{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Identification of Tumors in Brain MRI's via Classification in Convolutional Neural Networks**"],"metadata":{"id":"sw7sInbLYzWk"}},{"cell_type":"markdown","source":["# Import Libraries and Mount Google Drive"],"metadata":{"id":"UQYm1phlYw8R"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jhx87sZXSWc8"},"outputs":[],"source":["# Import necessary Libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import scipy.optimize as opt\n","\n","import sklearn\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","\n","from sklearn.model_selection import learning_curve\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold\n","\n","from sklearn.model_selection import KFold\n","\n","import os\n","import cv2 as cv\n","from PIL import Image\n","import pprint"]},{"cell_type":"code","source":["# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"],"metadata":{"id":"oGrb8NFQS3ZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Convolution and Pooling: Feature Matrix Output"],"metadata":{"id":"Mu5H73s7UF7u"}},{"cell_type":"code","source":["path_yes = '/content/drive/MyDrive/College/04 Senior/Machine Learning/Final Project/Dataset/yes'\n","\n","path_no = '/content/drive/MyDrive/College/04 Senior/Machine Learning/Final Project/Dataset/no'\n","\n","yes_imgs = os.listdir(path_yes)\n","\n","no_imgs = os.listdir(path_no)\n","\n","n_yes = len(yes_imgs)\n","\n","n_no = len(no_imgs)\n","\n","print(f'# Yes: {n_yes} # No: {n_no}  # Total: {n_yes+n_no}')"],"metadata":{"id":"SJ2KtcGNUPss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### Generate Yes Feature Matrix ####\n","\n","# Conv1 --> Pool1 --> Conv2 --> Pool2\n","\n","n_yes = len(yes_imgs)\n","\n","n_no = len(no_imgs)\n","\n","final_pixlength = 75*75\n","\n","yes_array = np.zeros((final_pixlength,n_yes))\n","no_array = np.zeros((final_pixlength,n_no))\n","\n","for i in range(n_yes):\n","\n","  img = Image.open(path_yes + '/' + yes_imgs[i])\n","\n","  image_data = np.array(img)\n","\n","  if len(image_data.shape) == 2:\n","    image_data = np.expand_dims(image_data,2)\n","\n","\n","  image_data = tf.image.resize(image_data,(300,300))\n","\n","  image_data = image_data[:,:,0]\n","\n","  kernel_n = 3\n","\n","  filter = np.array([[0,10,0],\n","                    [10,0,10],\n","                    [0,10,0]])\n","\n","  m,n= image_data.shape\n","\n","  skip = 0\n","\n","  conv1_shape = np.array([m,n])\n","\n","  conv1 = np.zeros((conv1_shape[0],conv1_shape[1]))\n","\n","  image_datap = np.pad(image_data,(1,1),'constant')\n","\n","  for k in range(conv1_shape[1]):\n","\n","    for j in range(conv1_shape[0]):\n","\n","      current_slice = image_datap[j:j+3,k:k+3]\n","\n","      #print(current_slice)\n","      \n","      conv1[j,k] = np.sum(current_slice*filter)\n","\n","\n","  m,n = conv1.shape\n","\n","  pool1 = np.zeros((int(m/2),int(n/2)))\n","\n","  for k in range(int(m/2)):\n","\n","    for j in range(int(n/2)):\n","\n","      pool1[k,j] = np.max(conv1[2*k:2*k + 2, 2*j:2*j+2])\n","\n","\n","  m,n = pool1.shape\n","\n","  conv2_shape = np.array([m,n])\n","\n","  conv2 = np.zeros((conv2_shape[0],conv2_shape[1]))\n","\n","  pool1p = np.pad(pool1,(1,1),'constant')\n","\n","  for l in range(conv2_shape[1]):\n","\n","    for p in range(conv2_shape[0]):\n","\n","      current_slice = pool1p[p:p+3,l:l+3]\n","\n","      #print(current_slice)\n","      \n","      conv2[p,l] = np.sum(current_slice*filter)\n","\n","  \n","\n","  m,n = conv2.shape\n","\n","  ###\n","\n","  pool2 = np.zeros((int(m/2),int(n/2)))\n","\n","  for k in range(int(m/2)):\n","\n","    for j in range(int(n/2)):\n","\n","      pool2[k,j] = np.max(conv2[2*k:2*k + 2, 2*j:2*j+2])\n","\n","\n","  yes_array[:,i] = pool2.flatten()"],"metadata":{"id":"BI_xhP0VUlh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Validate and Save Yes Data\n","print(yes_array.T.shape)\n","plt.plot(yes_array.T[8,:])"],"metadata":{"id":"18dpy223U0Tq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(yes_array).to_csv(\"/content/drive/MyDrive/College/04 Senior/Machine Learning/Final Project/Dataset/YesData300_10t.csv\")"],"metadata":{"id":"vzaL9IetWSpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### Generate No Feature Matrix ####\n","\n","# Conv1 --> Pool1 --> Conv2 --> Pool2\n","\n","for i in range(n_no):\n","\n","  img = Image.open(path_no + '/' + no_imgs[i])\n","\n","  image_data = np.array(img)\n","\n","  if len(image_data.shape) == 2:\n","    image_data = np.expand_dims(image_data,2)\n","\n","\n","  image_data = tf.image.resize(image_data,(300,300))\n","\n","  image_data = image_data[:,:,0]\n","\n","  kernel_n = 3\n","\n","  filter = np.array([[0,10,0],\n","                    [10,0,10],\n","                    [0,10,0]])\n","\n","  m,n= image_data.shape\n","\n","  skip = 0\n","\n","  conv1_shape = np.array([m,n])\n","\n","  conv1 = np.zeros((conv1_shape[0],conv1_shape[1]))\n","\n","  image_datap = np.pad(image_data,(1,1),'constant')\n","\n","  for k in range(conv1_shape[1]):\n","\n","    for j in range(conv1_shape[0]):\n","\n","      current_slice = image_datap[j:j+3,k:k+3]\n","\n","      #print(current_slice)\n","      \n","      conv1[j,k] = np.sum(current_slice*filter)\n","\n","\n","  m,n = conv1.shape\n","\n","  pool1 = np.zeros((int(m/2),int(n/2)))\n","\n","  for k in range(int(m/2)):\n","\n","    for j in range(int(n/2)):\n","\n","      pool1[k,j] = np.max(conv1[2*k:2*k + 2, 2*j:2*j+2])\n","\n","\n","  m,n = pool1.shape\n","\n","  conv2_shape = np.array([m,n])\n","\n","  conv2 = np.zeros((conv2_shape[0],conv2_shape[1]))\n","\n","  pool1p = np.pad(pool1,(1,1),'constant')\n","\n","  for l in range(conv2_shape[1]):\n","\n","    for p in range(conv2_shape[0]):\n","\n","      current_slice = pool1p[p:p+3,l:l+3]\n","\n","      #print(current_slice)\n","      \n","      conv2[p,l] = np.sum(current_slice*filter)\n","\n","  \n","\n","  m,n = conv2.shape\n","\n","  ###\n","\n","  pool2 = np.zeros((int(m/2),int(n/2)))\n","\n","  for k in range(int(m/2)):\n","\n","    for j in range(int(n/2)):\n","\n","      pool2[k,j] = np.max(conv2[2*k:2*k + 2, 2*j:2*j+2])\n","\n","\n","  no_array[:,i] = pool2.flatten()\n"],"metadata":{"id":"p3xupRGRU-ud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Validate and Save No Data\n","print(no_array.T.shape)\n","plt.plot(no_array.T[3,:])"],"metadata":{"id":"3z-P2AVoVIJy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(no_array).to_csv(\"/content/drive/MyDrive/College/04 Senior/Machine Learning/Final Project/Dataset/NoData300_10t.csv\")"],"metadata":{"id":"WyWF2SVpWQ3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compare original and Convolved Images\n","num = 6\n","\n","plt.imshow(no_array.T[num,:].reshape(75,75))\n","plt.show()\n","\n","img = Image.open(path_no + '/' + no_imgs[num])\n","plt.imshow(img)\n","plt.show()\n","\n","plt.imshow(yes_array.T[num,:].reshape(75,75))\n","plt.show()\n","\n","img = Image.open(path_yes + '/' + yes_imgs[num])\n","plt.imshow(img)\n","plt.show()"],"metadata":{"id":"wt8VTkwkVqXX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Featurization, Normalization, and Neural Network Training"],"metadata":{"id":"7djGiuyCUMgu"}},{"cell_type":"code","source":["# Import Feature Matrix From Selected Dataset\n","nodata = pd.read_csv(r'/content/drive/MyDrive/College/04 Senior/Machine Learning/Final Project/Dataset/NoData300_10.csv')\n","yesdata = pd.read_csv(r'/content/drive/MyDrive/College/04 Senior/Machine Learning/Final Project/Dataset/YesData300_10.csv')"],"metadata":{"id":"hO5ifu27Tr_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if orientation is correct\n","print(nodata.shape)\n","print(yesdata.shape)"],"metadata":{"id":"KKKvgfJcWKT1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reorient if necessary; Careful to only run once per desired transposition\n","nodata = nodata.T\n","yesdata = yesdata.T"],"metadata":{"id":"O83ncWXVWan3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Experimental flip of flattening axis\n","yesdatat = np.zeros((yesdata.shape))\n","for i in range(yesdata.shape[0]):\n","\n","  yesdatat[i,:] = np.array(yesdata.iloc[i,:]).reshape(20,20).T.flatten()\n","\n","nodatat = np.zeros((nodata.shape))\n","for i in range(nodata.shape[0]):\n","\n","  nodatat[i,:] = np.array(nodata.iloc[i,:]).reshape(20,20).T.flatten()\n","\n","yesdata = pd.DataFrame(yesdatat)\n","\n","nodata = pd.DataFrame(nodatat)"],"metadata":{"id":"l-Qo-3Yf27td"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize row vector symmetry\n","w = 6\n","\n","yesar = np.array(yesdata)\n","noar = np.array(nodata)\n","\n","plt.plot(yesar[w,:])\n","plt.show()\n","plt.plot(noar[w,:])\n","plt.show()\n"],"metadata":{"id":"Vp3POaQYdjgW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Construct feature matrix and label vector\n","\n","X_feat = nodata.copy()\n","X_feat = X_feat.append(yesdata,ignore_index=True).iloc[:,1:]\n","\n","X_feat.tail()\n","\n","yes_label = np.ones(len(yesdata.iloc[:,0]))\n","no_label = np.zeros(len(nodata.iloc[:,0]))\n","\n","y = np.append(no_label,yes_label)\n","\n","print(y,y.shape)"],"metadata":{"id":"FYqhsS8SWeXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Normalize features\n","means = X_feat.mean(axis=1)\n","stds = X_feat.std(axis=1)\n","\n","m,n = X_feat.shape\n","\n","X_norm = np.zeros((m,n))\n","\n","for i in range(m):\n","  X_norm[i,:] = (X_feat.iloc[i,:] - means[i]*np.ones(n))/ (stds[i]*np.ones(n))\n","\n","pd.DataFrame(X_norm.T).describe()"],"metadata":{"id":"__lj9CKYWl5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.30,stratify=y, random_state=42)\n","\n","# Validate Stratification\n","plt.plot(y_train,'ro',markersize=2)\n","plt.plot(y_test,'bo',markersize=2)"],"metadata":{"id":"eUAp7dnyWrz3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to build and adjust neural network/hyperparameters\n","\n","def construct_model():\n","\n","  NN = Sequential()\n","  NN.add(Dense(80,activation='relu',use_bias=False))\n","  #NN.add(Dense(40,activation='softmax',use_bias=False))\n","  NN.add(Dense(20,activation='softmax',use_bias=False))\n","  NN.add(Dense(5,activation='softmax',use_bias=False))\n","\n","  NN.add(Dense(1,activation='sigmoid'))\n","\n","  # Compile the model\n","  NN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","  return NN"],"metadata":{"id":"F05AxwPLW00C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train model and plot accuracy/loss\n","# Plot code from https://vitalflux.com/python-keras-learning-validation-curve-classification-model/#:~:text=Learning%20curve%20is%20used%20to,than%20training%20loss%20represents%20overfitting\n","\n","model = construct_model()\n","\n","history = model.fit(X_train, y_train, validation_data = (X_test,y_test),epochs=100, verbose=0)\n","\n","history_dict = history.history\n","\n","print(history_dict.keys())\n","loss_values = history_dict['loss']\n","val_loss_values = history_dict['val_loss']\n","accuracy = history_dict['accuracy']\n","val_accuracy = history_dict['val_accuracy']\n"," \n","epochs = range(1, len(loss_values) + 1)\n","fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n","\n","# Plot the model accuracy vs Epochs\n","\n","ax[0].plot(epochs, accuracy, 'bo', label='Training accuracy')\n","ax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n","ax[0].set_title('Training & Validation Accuracy', fontsize=16)\n","ax[0].set_xlabel('Epochs', fontsize=16)\n","ax[0].set_ylabel('Accuracy', fontsize=16)\n","ax[0].legend()\n","\n","# Plot the loss vs Epochs\n","\n","ax[1].plot(epochs, loss_values, 'bo', label='Training loss')\n","ax[1].plot(epochs, val_loss_values, 'b', label='Validation loss')\n","ax[1].set_title('Training & Validation Loss', fontsize=16)\n","ax[1].set_xlabel('Epochs', fontsize=16)\n","ax[1].set_ylabel('Loss', fontsize=16)\n","ax[1].legend()"],"metadata":{"id":"sRyEp6KXW6-L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Learning Curve"],"metadata":{"id":"XFJgmUeQYmbX"}},{"cell_type":"code","source":["# Learning Curve Function\n","\n","def learningcurve(X,y,X_val,y_val,m):\n","  '''\n","  Function to calculate training and validation error\n","  Inputs:\n","  X=training set features (assumes already has columns of 1)\n","  y=training set data\n","  X_val = validation features (assumes already has columns of 1)\n","  y_val = validation data\n","\n","  Output:\n","  [error_train, error_val] = array with training and validation errors as a function of the number of data points\n","  '''\n","###BEGIN SOLUTION\n","  \n","  assert (m <= len(y_val)), 'M is larger than the validation set'\n","\n","  error_train = np.zeros(m)\n","  error_val = np.zeros(m)\n","\n","  for i in range(1,m):\n","    \n","    history = model.fit(X_train[:m], y_train[:m], validation_data = (X_test,y_test),epochs=100, verbose=0)\n","\n","    history_dict = history.history\n","\n","    loss_values = history_dict['loss']\n","    val_loss_values = history_dict['val_loss']\n","\n","    error_train[i] = loss_values[-1]\n","    error_val[i] = val_loss_values[-1]\n","\n","  return [error_train,error_val]"],"metadata":{"id":"u5uX8qCZXZpo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate Learning Curves\n","\n","#error_train,error_val = learningcurve(X_train,y_train,X_test,y_test,50)\n","\n","plt.plot(error_train,'o-',label='Training Error')\n","plt.plot(error_val,'o-',label='Validation Error')\n","plt.legend(loc='best')\n","plt.xlabel('Number of Data Points Used')\n","plt.ylabel('Model Error')\n","plt.show()"],"metadata":{"id":"Nj3KbhNDXhdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot model accuracy\n","plt.plot(history.history['accuracy'])"],"metadata":{"id":"DdQyXHh_XwsC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Confusion Matrices"],"metadata":{"id":"KQbVoKOkYpfL"}},{"cell_type":"code","source":["# Confusion Matrix Training\n","\n","y_pred_train = model.predict(X_train)\n","\n","for i in range(len(y_pred_train)):\n","\n","  if y_pred_train[i] <0.5:\n","\n","    y_pred_train[i] = 0\n","\n","  elif y_pred_train[i] >=0.5:\n","\n","    y_pred_train[i] = 1\n","\n","print(y_pred_train.shape)\n","\n","#plt.plot(y_pred_train,'o')\n","plt.plot(y_train,y_pred_train,'o')\n","\n","pos = np.where(y_train==1)[0]\n","neg = np.where(y_train==0)[0]\n","\n","pos_pred = np.where(y_pred_train >= 0.5)[0]\n","neg_pred = np.where(y_pred_train < 0.5)[0]\n","\n","true_pos = len(np.intersect1d(pos,pos_pred))\n","\n","true_neg = len(np.intersect1d(neg,neg_pred))\n","\n","false_pos = len(np.intersect1d(neg,pos_pred))\n","\n","false_neg = len(np.intersect1d(pos,neg_pred))\n","\n","print(f'True Positive: {true_pos}, True Negative: {true_neg}, False Positive: {false_pos}, False Negative: {false_neg}.')\n","\n","print(f'Positive Identification Rate: {100*true_pos / (true_pos + false_neg): .2f}%')\n","\n","print('')\n","\n","print(f'Negative Identification Rate: {100*true_neg / (true_neg + false_pos): .2f}%')"],"metadata":{"id":"tETZdHaVXoxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confusion Matrix Test Data\n","\n","y_pred_test = model.predict(X_test)\n","\n","pos = np.where(y_test==1)[0]\n","neg = np.where(y_test==0)[0]\n","\n","for i in range(len(y_pred_test)):\n","\n","  if y_pred_test[i] <0.5:\n","\n","    y_pred_test[i] = 0\n","\n","  elif y_pred_test[i] >=0.5:\n","\n","    y_pred_test[i] = 1\n","\n","pos_pred = np.where(y_pred_test >= 0.5)[0]\n","neg_pred = np.where(y_pred_test < 0.5)[0]\n","\n","plt.plot(y_test,y_pred_test,'o')\n","plt.show()\n","\n","true_pos = len(np.intersect1d(pos,pos_pred))\n","\n","true_neg = len(np.intersect1d(neg,neg_pred))\n","\n","false_pos = len(np.intersect1d(neg,pos_pred))\n","\n","false_neg = len(np.intersect1d(pos,neg_pred))\n","\n","print(f'True Positive: {true_pos}, True Negative: {true_neg}, False Positive: {false_pos}, False Negative: {false_neg}.')\n","\n","print(f'Positive Identification Rate: {100*true_pos / (true_pos + false_neg): .2f}%')\n","\n","print('')\n","\n","print(f'Negative Identification Rate: {100*true_neg / (true_neg + false_pos): .2f}%')"],"metadata":{"id":"K3smX3i5X_6U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# K-Fold Validation"],"metadata":{"id":"3xnLIPe8YsMB"}},{"cell_type":"code","source":["# K Fold Validation\n","\n","# Code from https://vitalflux.com/k-fold-cross-validation-python-example/\n","\n","model = construct_model()\n","\n","strtfdKFold = StratifiedKFold(n_splits=10)\n","kfold = strtfdKFold.split(X_train, y_train)\n","scores = []\n","\n","for k, (train, test) in enumerate(kfold):\n","    model.fit(X_train[train, :], y_train[train])\n","    score = model.evaluate(X_train[test, :], y_train[test])[0]\n","    scores.append(score)\n","    print('Fold: %2d, Accuracy: %.3f' % (k+1, score))\n"," \n","print('\\n\\nCross-Validation accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))"],"metadata":{"id":"G6qf2ZKWX51v"},"execution_count":null,"outputs":[]}]}